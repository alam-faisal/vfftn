{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/alam/code/vfftn/src')\n",
    "from models2 import heisenberg_ham, kronecker_pad, xy_ham \n",
    "from mpo import pauli\n",
    "import numpy as np \n",
    "from scipy.special import comb\n",
    "from functools import reduce\n",
    "from scipy.optimize import minimize, linear_sum_assignment, dual_annealing\n",
    "from itertools import permutations, combinations\n",
    "from tqdm.notebook import tqdm \n",
    "import time \n",
    "\n",
    "# xxx model\n",
    "xxx_ham = lambda n : heisenberg_ham(n, jx=1/4,jy=1/4,jz=1/4,h=0)  \n",
    "xxz_ham = lambda n : heisenberg_ham(n, jx=1/4,jy=1/4,jz=1,h=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sz(n): \n",
    "    total_z = np.zeros((2**n,2**n), dtype=complex)\n",
    "    for i in range(n): \n",
    "        total_z += kronecker_pad(pauli[3], n, i)\n",
    "    return total_z\n",
    "\n",
    "def total_spin(n): \n",
    "    total_x = np.zeros((2**n,2**n), dtype=complex)\n",
    "    total_y = np.zeros((2**n,2**n), dtype=complex)\n",
    "    total_z = np.zeros((2**n,2**n), dtype=complex)\n",
    "    for i in range(n):\n",
    "        total_x += kronecker_pad(pauli[1], n, i) \n",
    "        total_y += kronecker_pad(pauli[2], n, i)\n",
    "        total_z += kronecker_pad(pauli[3], n, i)\n",
    "\n",
    "    return [total_x, total_y, total_z]\n",
    "\n",
    "def total_spin_squared(n):\n",
    "    total_x, total_y, total_z = total_spin(n)\n",
    "    return total_x @ total_x + total_y @ total_y + total_z @ total_z\n",
    "\n",
    "def fixed_sz_basis(n, f):\n",
    "    if f < 0 or f > n:\n",
    "        return []\n",
    "    \n",
    "    result = []\n",
    "    for positions in combinations(range(n), f):\n",
    "        binary = ['0'] * n\n",
    "        for pos in positions:\n",
    "            binary[pos] = '1'\n",
    "        result.append(''.join(binary))\n",
    "    return result\n",
    "\n",
    "def bin_to_vec(b): \n",
    "    vec = np.zeros(2**len(b))\n",
    "    vec[int(b,2)] = 1.0\n",
    "    return vec\n",
    "\n",
    "def u1_projector(n,m):\n",
    "    basis = fixed_sz_basis(n,m) \n",
    "    return np.array([bin_to_vec(b) for b in basis])\n",
    "\n",
    "def su2_multiplicity(n,j):\n",
    "    \"\"\" returns the multiplicity of the j-th irrep of n tensor products of su(2) \"\"\"\n",
    "    return int(comb(n,int(n/2-j)) - comb(n, int(n/2-j)-1)) \n",
    "\n",
    "def su2_dimension(j): \n",
    "    return int(2*j + 1)  \n",
    "\n",
    "def su2_projector(n,j,m): \n",
    "    \"\"\" returns the projector onto the subspace with total spin j and total z-component m \"\"\"\n",
    "    U = np.loadtxt(f'N={n}.txt', delimiter=',')\n",
    "    \n",
    "    j_cur = 0 if n % 2 == 0 else 1/2\n",
    "    sector_start = 0 \n",
    "    while j_cur < j: \n",
    "        sector_start += su2_multiplicity(n,j_cur) * su2_dimension(j_cur)\n",
    "        j_cur += 1\n",
    "\n",
    "    m_offset = int((-m * 2 + 2*j)/2) if (m*2)%2 == 1 else (-m+j)\n",
    "    indices = []\n",
    "    for i in range(su2_multiplicity(n,j)):\n",
    "        indices.append(sector_start + i*su2_dimension(j) + m_offset)\n",
    "\n",
    "    P = np.zeros((2**n,2**n), dtype=complex)\n",
    "    for index in indices: \n",
    "        P += np.outer(U[:,index], U[:,index].conj())\n",
    "    return P, U[:,indices] \n",
    "\n",
    "def nn_ansatz(params, n): \n",
    "    \"\"\" len(params) = 2n \"\"\"\n",
    "    ansatz = np.zeros((2**n,2**n), dtype=complex)\n",
    "\n",
    "    for i in range(n):\n",
    "        ansatz += params[i] * kronecker_pad(pauli[3], n, i) \n",
    "\n",
    "    ZZ = np.kron(pauli[3], pauli[3])\n",
    "    for i in range(n-1):\n",
    "        ansatz += params[n+i] * kronecker_pad(ZZ, n, i)\n",
    "        \n",
    "    ansatz += params[-1] * np.eye(2**n)\n",
    "    return ansatz \n",
    "\n",
    "def one_local_ansatz(params, n):\n",
    "    \"\"\" len(params) = n + 1 \"\"\"\n",
    "    ansatz = np.zeros((2**n,2**n), dtype=complex)\n",
    "\n",
    "    for i in range(n):\n",
    "        ansatz += params[i] * kronecker_pad(pauli[3], n, i) \n",
    "\n",
    "    ansatz += params[-1] * np.eye(2**n)\n",
    "    return ansatz\n",
    "\n",
    "def two_local_ansatz(params, n): \n",
    "    \"\"\" len(params) = n + n(n-1)/2 + 1 \"\"\"\n",
    "    ansatz = np.zeros((2**n,2**n), dtype=complex)\n",
    "\n",
    "    for i in range(n):\n",
    "        ansatz += params[i] * kronecker_pad(pauli[3], n, i) \n",
    "\n",
    "    # generates pairs of sites from 0 to n-1\n",
    "    sites = [(i,j) for i in range(n) for j in range(i+1,n)]\n",
    "    for param_count, (i,j) in enumerate(sites): \n",
    "        kron_list = [pauli[3] if k == i or k == j else np.eye(2) for k in range(n)]\n",
    "        ansatz += params[n + param_count] * reduce(np.kron, kron_list)\n",
    "\n",
    "    ansatz += params[-1] * np.eye(2**n)\n",
    "    return ansatz \n",
    "\n",
    "def unitary_equivalence(D, H):\n",
    "    \"\"\"\n",
    "    Finds a unitary matrix U such that U D U^† = H. D and H are normal matrices with the same eigenvalues. \n",
    "    \"\"\"\n",
    "    # Perform eigenvalue decomposition for D and H\n",
    "    eigvals_D, V_D = np.linalg.eigh(D)  # eigenvalues and eigenvectors of D\n",
    "    eigvals_H, V_H = np.linalg.eigh(H)  # eigenvalues and eigenvectors of H\n",
    "    \n",
    "    # Ensure eigenvalues are sorted consistently (since they might not be in order)\n",
    "    idx_D = np.argsort(eigvals_D)\n",
    "    idx_H = np.argsort(eigvals_H)\n",
    "    \n",
    "    V_D = V_D[:, idx_D]\n",
    "    V_H = V_H[:, idx_H]\n",
    "    \n",
    "    # Construct U\n",
    "    U = V_H @ V_D.conj().T\n",
    "    \n",
    "    return U\n",
    "\n",
    "def nn_cost(params, n, target_evs):\n",
    "    ansatz = basis.conj().T @ nn_ansatz(params, n) @ basis\n",
    "    ansatz_evals = np.linalg.eigvalsh(ansatz)\n",
    "    return np.linalg.norm(ansatz_evals - target_evs)\n",
    "\n",
    "def two_local_cost(params, n, target_evs): \n",
    "    ansatz = basis.conj().T @ two_local_ansatz(params, n) @ basis\n",
    "    ansatz_evals = np.linalg.eigvalsh(ansatz)\n",
    "    return np.linalg.norm(ansatz_evals - target_evs)\n",
    "\n",
    "def generate_cost_matrix(params, target, n, ansatz_func=nn_ansatz):\n",
    "    ansatz = np.diag(ansatz_func(params, n))\n",
    "    cost_function = lambda a, t: abs(a - t)\n",
    "    cost_matrix = np.array([[cost_function(a, t) for t in target] for a in ansatz])\n",
    "    \n",
    "    if len(ansatz) > len(target):\n",
    "        dummy_columns = np.zeros((len(ansatz), len(ansatz) - len(target)))\n",
    "        cost_matrix = np.hstack((cost_matrix, dummy_columns))\n",
    "\n",
    "    return cost_matrix\n",
    "\n",
    "def hungarian_cost(params, target, n, ansatz_func=nn_ansatz, quiet=True):\n",
    "    cost_matrix = generate_cost_matrix(params, target, n, ansatz_func)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    min_cost = cost_matrix[row_ind, col_ind].sum()\n",
    "\n",
    "    global last_print_time\n",
    "    if time.time() - last_print_time >= 5:\n",
    "        print(f\"Cost: {min_cost}\")\n",
    "        last_print_time = time.time()\n",
    "\n",
    "    if not quiet: \n",
    "        print(\"Optimal Assignments:\")\n",
    "        for worker, task in zip(row_ind, col_ind):\n",
    "            if task < len(target): \n",
    "                print(f\"Worker {worker} → Task {task} (Cost: {cost_matrix[worker, task]})\")\n",
    "        print(f\"\\nTotal Minimum Cost: {min_cost}\") \n",
    "\n",
    "    return min_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XY model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "12\n",
      "Cost: 2.9795631387720016\n",
      "Cost: 3.7522608572142846\n",
      "Cost: 3.1894395608322035\n",
      "Cost: 2.3143374697586925\n",
      "Cost: 3.3471735479677363\n",
      "Cost: 2.1633172440307695\n",
      "Cost: 0.5626008391821458\n",
      "Cost: 5.888126894846916\n",
      "Optimal Assignments:\n",
      "Worker 1 → Task 11 (Cost: 0.00030913152251099696)\n",
      "Worker 2 → Task 19 (Cost: 0.004224612919776938)\n",
      "Worker 6 → Task 18 (Cost: 8.815140359885731e-05)\n",
      "Worker 7 → Task 14 (Cost: 0.0005075423279365943)\n",
      "Worker 16 → Task 12 (Cost: 6.264001407407793e-05)\n",
      "Worker 19 → Task 7 (Cost: 0.17171244165119848)\n",
      "Worker 22 → Task 10 (Cost: 2.3705949127178494e-05)\n",
      "Worker 23 → Task 4 (Cost: 0.0004430968734685514)\n",
      "Worker 25 → Task 2 (Cost: 0.004799194079445179)\n",
      "Worker 26 → Task 15 (Cost: 0.00026544963716368386)\n",
      "Worker 27 → Task 9 (Cost: 0.00015394128717899358)\n",
      "Worker 28 → Task 17 (Cost: 0.00018717198527928858)\n",
      "Worker 32 → Task 16 (Cost: 0.004713575886991173)\n",
      "Worker 35 → Task 13 (Cost: 7.137748327945559e-05)\n",
      "Worker 48 → Task 3 (Cost: 0.0005777436210694376)\n",
      "Worker 55 → Task 0 (Cost: 7.200673352958376e-05)\n",
      "Worker 58 → Task 6 (Cost: 0.0007805532441547136)\n",
      "Worker 60 → Task 8 (Cost: 0.0007022755922747592)\n",
      "Worker 62 → Task 5 (Cost: 0.0016693141658694621)\n",
      "Worker 63 → Task 1 (Cost: 0.002088705090210752)\n",
      "\n",
      "Total Minimum Cost: 0.19345263146813815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19345263146813815"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing one-local ansatz with Hungarian algorithm + simulated annealing\n",
    "n = 6\n",
    "ham = xy_ham(n)\n",
    "proj = u1_projector(n,3) \n",
    "print(proj.shape[0])  \n",
    "\n",
    "eff_ham = proj @ ham @ proj.conj().T\n",
    "target_evals = np.linalg.eigvalsh(eff_ham)\n",
    "\n",
    "#ansatz_func = one_local_ansatz\n",
    "#bounds = [(-4, 4)] * (n+1)  # need to be expanded\n",
    "ansatz_func = nn_ansatz\n",
    "bounds = [(-2, 2)] * (2*n)  # need to be expanded\n",
    "print(len(bounds)) \n",
    "quiet=True\n",
    "\n",
    "last_print_time = time.time()\n",
    "result = dual_annealing(lambda params: hungarian_cost(params, target_evals, n, ansatz_func, quiet), bounds)\n",
    "\n",
    "hungarian_cost(result.x, target_evals, n, ansatz_func, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.98791841, -5.20775094, -3.60387547, -3.60387547, -2.49395921,\n",
       "       -2.49395921, -2.        , -0.89008374, -0.89008374, -0.21983253,\n",
       "        0.21983253,  0.89008374,  0.89008374,  2.        ,  2.49395921,\n",
       "        2.49395921,  3.60387547,  3.60387547,  5.20775094,  6.98791841])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_evals "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXZ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "Cost: 0.03724477697744134\n",
      "Cost: 2.3747850400494093\n",
      "Cost: 3.8956419047811295\n",
      "Optimal Assignments:\n",
      "Worker 1 → Task 6 (Cost: 3.8032238552920816e-08)\n",
      "Worker 2 → Task 8 (Cost: 6.883908283850104e-05)\n",
      "Worker 5 → Task 9 (Cost: 0.006602111405924482)\n",
      "Worker 9 → Task 1 (Cost: 2.2790452325338606e-07)\n",
      "Worker 16 → Task 4 (Cost: 0.0026632550998686444)\n",
      "Worker 19 → Task 3 (Cost: 3.02438474264477e-09)\n",
      "Worker 20 → Task 7 (Cost: 0.01212207255175346)\n",
      "Worker 25 → Task 0 (Cost: 0.009510228872446014)\n",
      "Worker 26 → Task 2 (Cost: 0.0025856320373778274)\n",
      "Worker 28 → Task 5 (Cost: 1.0608901188091657e-08)\n",
      "\n",
      "Total Minimum Cost: 0.033552418620256666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.033552418620256666"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing NN ansatz with Hungarian algorithm + simulated annealing\n",
    "n = 5\n",
    "ham = xxz_ham(n)\n",
    "proj = u1_projector(n,2) \n",
    "print(proj.shape[0])  \n",
    "\n",
    "eff_ham = proj @ ham @ proj.conj().T\n",
    "target_evals = np.linalg.eigvalsh(eff_ham)\n",
    "\n",
    "ansatz_func = nn_ansatz\n",
    "bounds = [(-1, 1)] * (2 * n)  # need to be expanded\n",
    "print(len(bounds)) \n",
    "quiet=True\n",
    "\n",
    "last_print_time = time.time()\n",
    "result = dual_annealing(lambda params: hungarian_cost(params, target_evals, n, ansatz_func, quiet), bounds)\n",
    "\n",
    "hungarian_cost(result.x, target_evals, n, ansatz_func, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "22\n",
      "Cost: 0.4418724781829984\n",
      "Cost: 2.0424634298163156\n",
      "Cost: 0.26015901570066574\n",
      "Cost: 8.462257227577451\n",
      "Cost: 16.53909740803774\n",
      "Cost: 6.070875353021597\n",
      "Cost: 17.030109069661755\n",
      "Cost: 7.643102796626452\n",
      "Cost: 5.049878092816561\n",
      "Cost: 16.258769051284318\n",
      "Cost: 5.8484467036040035\n",
      "Cost: 28.572171342700514\n",
      "Cost: 7.170617133651238\n",
      "Cost: 10.368949031320785\n",
      "Cost: 5.3018302001188164\n",
      "Cost: 17.219806439974043\n",
      "Cost: 13.223656411209125\n",
      "Cost: 4.681321268031265\n",
      "Cost: 6.4549837952607785\n",
      "Cost: 16.56673856777312\n",
      "Cost: 21.568749505439648\n",
      "Cost: 10.243655433189888\n",
      "Cost: 1.7924908731159777\n",
      "Cost: 12.426817991990738\n",
      "Cost: 11.224337416997177\n",
      "Cost: 0.5663217527580267\n",
      "Cost: 0.4220915095237057\n",
      "Cost: 20.822467747222333\n",
      "Optimal Assignments:\n",
      "Worker 6 → Task 11 (Cost: 0.00025843841672068546)\n",
      "Worker 9 → Task 5 (Cost: 0.0008420206212145231)\n",
      "Worker 13 → Task 0 (Cost: 0.00811692109021589)\n",
      "Worker 14 → Task 19 (Cost: 0.03406201536933251)\n",
      "Worker 19 → Task 9 (Cost: 2.2432035051034305e-05)\n",
      "Worker 27 → Task 17 (Cost: 0.006075615898163189)\n",
      "Worker 28 → Task 2 (Cost: 4.487520305485049e-05)\n",
      "Worker 30 → Task 8 (Cost: 8.236065094147094e-06)\n",
      "Worker 32 → Task 18 (Cost: 8.614917559812696e-05)\n",
      "Worker 35 → Task 1 (Cost: 0.0001306477894145175)\n",
      "Worker 40 → Task 7 (Cost: 0.000117532657982089)\n",
      "Worker 41 → Task 14 (Cost: 2.4552455092585745e-05)\n",
      "Worker 45 → Task 3 (Cost: 0.01446682551260059)\n",
      "Worker 46 → Task 12 (Cost: 4.224032555749657e-05)\n",
      "Worker 47 → Task 4 (Cost: 0.004464858399694016)\n",
      "Worker 48 → Task 6 (Cost: 0.002159956983016187)\n",
      "Worker 49 → Task 13 (Cost: 0.018043758258602605)\n",
      "Worker 52 → Task 16 (Cost: 8.526502870997987e-05)\n",
      "Worker 58 → Task 15 (Cost: 0.0001649546306916605)\n",
      "Worker 59 → Task 10 (Cost: 0.0033567311199897976)\n",
      "\n",
      "Total Minimum Cost: 0.09257402703579648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09257402703579648"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing two-local ansatz with Hungarian algorithm + simulated annealing\n",
    "n = 6\n",
    "ham = xxz_ham(n)\n",
    "proj = u1_projector(n,3) \n",
    "print(proj.shape[0])  \n",
    "\n",
    "eff_ham = proj @ ham @ proj.conj().T\n",
    "target_evals = np.linalg.eigvalsh(eff_ham)\n",
    "\n",
    "ansatz_func = two_local_ansatz\n",
    "bounds = [(-5, 5)] * (n + 1 + int(n*(n-1)/2))\n",
    "print(len(bounds)) \n",
    "quiet=True\n",
    "\n",
    "last_print_time = time.time()\n",
    "result = dual_annealing(lambda params: hungarian_cost(params, target_evals, n, ansatz_func, quiet), bounds)\n",
    "\n",
    "hungarian_cost(result.x, target_evals, n, ansatz_func, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.000000000000002+0j) (2.000000000000001+0j)\n",
      "(8.000000000000018+0j) (2.0000000000000044+0j)\n",
      "(8.000000000000004+0j) (2.0000000000000013+0j)\n",
      "(7.999999999999995+0j) (1.9999999999999996+0j)\n",
      "(8+0j) (2+0j)\n",
      "(7.999999999999992+0j) (1.9999999999999982+0j)\n",
      "(8.00000000000001+0j) (2.000000000000003+0j)\n",
      "(7.999999999999992+0j) (1.9999999999999982+0j)\n",
      "(8.000000000000007+0j) (2.0000000000000013+0j)\n",
      "(7.999999999999997+0j) (1.9999999999999987+0j)\n",
      "(7.999999999999999+0j) (1.9999999999999991+0j)\n",
      "(7.999999999999995+0j) (1.999999999999999+0j)\n",
      "(7.999999999999997+0j) (1.9999999999999993+0j)\n",
      "(7.999999999999999+0j) (2+0j)\n",
      "(8+0j) (1.9999999999999998+0j)\n",
      "(8.000000000000012+0j) (2.0000000000000027+0j)\n",
      "(7.99999999999997+0j) (1.9999999999999925+0j)\n",
      "(8.000000000000012+0j) (2.0000000000000027+0j)\n",
      "(8.000000000000014+0j) (2.0000000000000036+0j)\n",
      "(8.00000000000001+0j) (2.000000000000002+0j)\n",
      "(7.99999999999997+0j) (1.9999999999999925+0j)\n",
      "(8.000000000000012+0j) (2.000000000000003+0j)\n",
      "(8.000000000000005+0j) (2.0000000000000013+0j)\n",
      "(8.000000000000012+0j) (2.0000000000000027+0j)\n",
      "(8.000000000000014+0j) (2.000000000000003+0j)\n",
      "(8.000000000000014+0j) (2.0000000000000036+0j)\n",
      "(8.00000000000001+0j) (2.000000000000003+0j)\n",
      "(8.000000000000005+0j) (2.0000000000000018+0j)\n",
      "(7.9999999999999964+0j) (1.9999999999999991+0j)\n",
      "(7.999999999999995+0j) (1.9999999999999978+0j)\n",
      "(8.000000000000004+0j) (2.000000000000001+0j)\n",
      "(8.000000000000002+0j) (2+0j)\n",
      "(7.999999999999999+0j) (2.0000000000000004+0j)\n",
      "(7.999999999999993+0j) (1.9999999999999991+0j)\n",
      "(8.000000000000002+0j) (2+0j)\n",
      "(7.999999999999998+0j) (2+0j)\n",
      "(7.999999999999996+0j) (1.9999999999999984+0j)\n",
      "(8+0j) (2+0j)\n",
      "(8.000000000000007+0j) (2.0000000000000018+0j)\n",
      "(8.000000000000004+0j) (2+0j)\n",
      "(7.999999999999988+0j) (1.9999999999999964+0j)\n",
      "(8.000000000000004+0j) (2.000000000000001+0j)\n",
      "(8.000000000000007+0j) (2.000000000000002+0j)\n",
      "(7.999999999999996+0j) (2+0j)\n",
      "(8+0j) (2+0j)\n",
      "(8.00000000000001+0j) (2.000000000000002+0j)\n",
      "(7.9999999999999964+0j) (1.9999999999999991+0j)\n",
      "(8.000000000000004+0j) (2.0000000000000018+0j)\n",
      "(8+0j) (2+0j)\n",
      "(7.999999999999984+0j) (1.9999999999999964+0j)\n",
      "(8.000000000000009+0j) (2.0000000000000018+0j)\n",
      "(8.000000000000025+0j) (2.0000000000000058+0j)\n",
      "(8.000000000000007+0j) (2.0000000000000027+0j)\n",
      "(7.999999999999984+0j) (1.9999999999999978+0j)\n",
      "(8.00000000000001+0j) (2.0000000000000018+0j)\n",
      "(7.999999999999996+0j) (1.9999999999999991+0j)\n",
      "(8.000000000000002+0j) (2+0j)\n",
      "(8.000000000000025+0j) (2.0000000000000058+0j)\n",
      "(8.000000000000007+0j) (2.0000000000000027+0j)\n",
      "(7.9999999999999964+0j) (1.9999999999999991+0j)\n",
      "(8.000000000000007+0j) (2.0000000000000027+0j)\n",
      "(8+0j) (2.0000000000000004+0j)\n",
      "(8.00000000000001+0j) (2.0000000000000018+0j)\n",
      "(7.999999999999986+0j) (1.999999999999997+0j)\n",
      "(8.000000000000004+0j) (2.000000000000001+0j)\n",
      "(8.000000000000007+0j) (2.0000000000000013+0j)\n",
      "(8.000000000000014+0j) (2.000000000000002+0j)\n",
      "(7.999999999999995+0j) (1.9999999999999982+0j)\n",
      "(8.00000000000002+0j) (2.0000000000000058+0j)\n",
      "(7.999999999999993+0j) (1.9999999999999982+0j)\n",
      "(8.000000000000004+0j) (2.0000000000000004+0j)\n",
      "(7.999999999999995+0j) (1.9999999999999991+0j)\n",
      "(8.000000000000005+0j) (2.0000000000000013+0j)\n",
      "(8.00000000000001+0j) (2.000000000000003+0j)\n",
      "(8.00000000000001+0j) (2.000000000000003+0j)\n",
      "(8+0j) (2+0j)\n",
      "(7.999999999999986+0j) (1.9999999999999964+0j)\n",
      "(8.000000000000007+0j) (2.000000000000002+0j)\n",
      "(7.999999999999993+0j) (1.9999999999999987+0j)\n",
      "(8.000000000000009+0j) (2.000000000000002+0j)\n",
      "(7.999999999999998+0j) (2+0j)\n",
      "(8.000000000000007+0j) (2.0000000000000018+0j)\n",
      "(7.999999999999993+0j) (1.9999999999999991+0j)\n",
      "(8.000000000000002+0j) (1.9999999999999998+0j)\n",
      "(8.000000000000016+0j) (2.000000000000003+0j)\n",
      "(8.000000000000009+0j) (2.0000000000000027+0j)\n",
      "(7.9999999999999964+0j) (1.9999999999999991+0j)\n",
      "(8.000000000000004+0j) (2.000000000000001+0j)\n",
      "(8+0j) (2.0000000000000004+0j)\n",
      "(7.999999999999987+0j) (1.999999999999997+0j)\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "# Can I generate subspaces with fixed total S and total Sz\n",
    "n = 10\n",
    "j = 1\n",
    "m = 1    \n",
    "\n",
    "Sz = total_sz(n)\n",
    "S_sq = total_spin_squared(n)\n",
    "P, basis = projector(n,j,m)\n",
    "for i in range(basis.shape[1]): \n",
    "    print(basis[:,i].conj().T @ S_sq @ basis[:,i], basis[:,i].conj().T @ Sz @ basis[:,i]) \n",
    "\n",
    "print(basis.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching over all permutations with nn_ansatz\n",
    "eff_ham = basis.conj().T @ xxx_ham(n) @ basis\n",
    "target_evals = np.linalg.eigvalsh(eff_ham)\n",
    "\n",
    "random_perms = [np.random.permutation(target_evals) for _ in range(100)]\n",
    "for perm in tqdm(random_perms):     \n",
    "    params = np.random.rand(2*n) \n",
    "    res = minimize(nn_cost, params, args=(n, perm), method='Powell')\n",
    "    print(nn_cost(res.x, n, target_evals)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing two-local ansatz\n",
    "def create_callback(n, target_evs):\n",
    "    def callback(xk):\n",
    "        current_cost = two_local_cost(xk, n, target_evs)\n",
    "        print(f\"Current cost: {current_cost}\")\n",
    "    return callback\n",
    "\n",
    "initial_params = np.random.rand(n+1 + int(n*(n-1)/2))\n",
    "\n",
    "result = minimize(\n",
    "    two_local_cost, \n",
    "    initial_params, \n",
    "    args=(n, target_evals),  # Pass additional variables to the cost function\n",
    "    method='Powell', \n",
    "    callback=create_callback(n, target_evals)  # Pass closure to include extra variables\n",
    ")\n",
    "print(result.fun) # Final cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.09262098581960737\n",
      "Optimal Assignments:\n",
      "Worker 5 → Task 25 (Cost: 0.003770667097336089)\n",
      "Worker 6 → Task 19 (Cost: 1.784267581592114e-05)\n",
      "Worker 7 → Task 21 (Cost: 0.0022501796005280905)\n",
      "Worker 9 → Task 8 (Cost: 0.001649771949137513)\n",
      "Worker 15 → Task 16 (Cost: 5.2019330509800454e-08)\n",
      "Worker 31 → Task 6 (Cost: 3.196889697565375e-05)\n",
      "Worker 39 → Task 3 (Cost: 3.4052183295329996e-10)\n",
      "Worker 53 → Task 26 (Cost: 0.002384016764500174)\n",
      "Worker 54 → Task 20 (Cost: 0.0015608858789831759)\n",
      "Worker 61 → Task 10 (Cost: 0.0008266220733800234)\n",
      "Worker 72 → Task 7 (Cost: 0.006688411924830051)\n",
      "Worker 74 → Task 1 (Cost: 0.0010917730314448093)\n",
      "Worker 79 → Task 2 (Cost: 0.00020987917019121838)\n",
      "Worker 81 → Task 4 (Cost: 0.0015706869454252814)\n",
      "Worker 109 → Task 0 (Cost: 0.006338038851148475)\n",
      "Worker 124 → Task 18 (Cost: 0.0017261810266968325)\n",
      "Worker 130 → Task 22 (Cost: 1.596253462743391e-09)\n",
      "Worker 149 → Task 27 (Cost: 0.022595465715853802)\n",
      "Worker 160 → Task 11 (Cost: 0.0002626121957685079)\n",
      "Worker 168 → Task 5 (Cost: 0.0010682565226132201)\n",
      "Worker 189 → Task 14 (Cost: 0.018913238461632342)\n",
      "Worker 192 → Task 24 (Cost: 1.680201300313655e-08)\n",
      "Worker 194 → Task 15 (Cost: 1.6346992682825245e-06)\n",
      "Worker 198 → Task 13 (Cost: 2.1696531849468847e-05)\n",
      "Worker 199 → Task 17 (Cost: 0.0038496393081463576)\n",
      "Worker 209 → Task 12 (Cost: 0.0035326624580670485)\n",
      "Worker 229 → Task 9 (Cost: 0.0012086683248502261)\n",
      "Worker 246 → Task 23 (Cost: 0.011050114957045998)\n",
      "\n",
      "Total Minimum Cost: 0.09262098581960737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09262098581960737"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing Hungarian algorithm + simulated annealing with nn_ansatz\n",
    "n = 8\n",
    "j = 1\n",
    "m = 1  \n",
    "\n",
    "Sz = total_sz(n)\n",
    "S_sq = total_spin_squared(n)\n",
    "P, basis = projector(n,j,m)\n",
    "\n",
    "print(basis.shape[1]) \n",
    "eff_ham = basis.conj().T @ xxx_ham(n) @ basis\n",
    "target_evals = np.linalg.eigvalsh(eff_ham)\n",
    "\n",
    "ansatz_func = nn_ansatz\n",
    "bounds = [(-1, 1)] * (2 * n)  # need to be expanded\n",
    "quiet=True\n",
    "\n",
    "last_print_time = time.time()\n",
    "result = dual_annealing(lambda params: hungarian_cost(params, target_evals, n, ansatz_func, quiet), bounds)\n",
    "\n",
    "hungarian_cost(result.x, target_evals, n, ansatz_func, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "56\n",
      "Cost: 0.42288255372305394\n",
      "Cost: 0.3967059803962218\n",
      "Cost: 0.45729154608396755\n",
      "Cost: 0.45833032406636964\n",
      "Cost: 0.41805459806695766\n",
      "Cost: 0.3979562861511392\n",
      "Cost: 0.5731431682058169\n",
      "Cost: 0.42025442501184107\n",
      "Cost: 0.5215228086617409\n",
      "Cost: 0.40889933586704097\n",
      "Cost: 0.40103917719638094\n",
      "Cost: 0.4058964376747088\n",
      "Cost: 0.3386527858589832\n",
      "Cost: 0.3386527458589834\n",
      "Cost: 0.3386528458589828\n",
      "Cost: 0.3386526658589839\n",
      "Cost: 0.3386528658589827\n",
      "Cost: 0.33865272585898354\n",
      "Cost: 2.6289778178152994\n",
      "Cost: 2.6289775678152796\n",
      "Cost: 2.6289775278152994\n",
      "Cost: 2.628977847815292\n",
      "Cost: 2.6289780278152928\n",
      "Cost: 2.6289779278152987\n",
      "Cost: 2.6289777478152985\n",
      "Cost: 0.4544300489411879\n",
      "Cost: 0.45442996894118837\n",
      "Cost: 0.4544299689411879\n",
      "Cost: 0.45443000894119034\n",
      "Cost: 0.4544298689411945\n",
      "Cost: 0.45443012894119705\n",
      "Cost: 0.4160336066605718\n",
      "Cost: 0.4160334666605749\n",
      "Cost: 0.41603368666057133\n",
      "Cost: 0.4160335066605724\n",
      "Cost: 0.41603346666057334\n",
      "Cost: 0.4160336066605755\n",
      "Cost: 0.5113190007496802\n",
      "Cost: 0.5113190307496787\n",
      "Cost: 0.511319150749677\n",
      "Cost: 0.5113189907496802\n",
      "Cost: 0.5113191307496805\n",
      "Cost: 0.5113190107496798\n",
      "Cost: 0.5113191907496825\n",
      "Cost: 0.4058420050994648\n",
      "Cost: 0.40584204509946364\n",
      "Cost: 0.4058419450994638\n",
      "Cost: 0.405842005099465\n",
      "Cost: 0.4058420850994654\n",
      "Cost: 0.4058418850994656\n",
      "Cost: 0.46427703246220053\n",
      "Cost: 0.46427687246219973\n",
      "Cost: 0.4642768124621992\n",
      "Cost: 0.4642768924622016\n",
      "Cost: 0.4642768724622053\n",
      "Cost: 0.4642769124622017\n",
      "Cost: 0.4728131629587925\n",
      "Cost: 0.47281303295879773\n",
      "Cost: 0.47281311295879347\n",
      "Cost: 0.47281307295879704\n",
      "Cost: 0.47281327295879494\n",
      "Cost: 0.47281311295879325\n",
      "Cost: 0.4728131929587922\n",
      "Cost: 0.3437032449409336\n",
      "Cost: 0.3437033249409389\n",
      "Cost: 0.3437032249409324\n",
      "Cost: 0.343703384940933\n",
      "Cost: 0.3437032049409374\n",
      "Cost: 0.3437031649409359\n",
      "Cost: 0.32472673356052584\n",
      "Cost: 0.3247269335605264\n",
      "Cost: 0.3247267935605299\n",
      "Cost: 0.32472687356053276\n",
      "Cost: 0.3247268535605311\n",
      "Cost: 0.3247268335605299\n",
      "Cost: 0.29773065401649534\n",
      "Cost: 0.29773068401649605\n",
      "Cost: 0.2977306240164991\n",
      "Cost: 0.29773066401649706\n",
      "Cost: 0.2977307640164958\n",
      "Cost: 0.2977305040164956\n",
      "Cost: 0.29773064401649374\n",
      "Cost: 0.30643665944503407\n",
      "Cost: 0.3064367994450381\n",
      "Cost: 0.3064369194450387\n",
      "Cost: 0.3064369394450399\n",
      "Cost: 0.3064370794450395\n",
      "Cost: 0.3064369394450376\n",
      "Cost: 0.28826734177000535\n",
      "Cost: 0.2882673417700098\n",
      "Cost: 0.2882674417700132\n",
      "Cost: 0.2882673017700129\n",
      "Cost: 0.28826712177001224\n",
      "Cost: 0.28826736177001455\n",
      "Cost: 0.2746186352201081\n",
      "Cost: 0.2746186252201064\n",
      "Cost: 0.2746185652201094\n",
      "Cost: 0.2746187052201068\n",
      "Cost: 0.2746185652201081\n",
      "Cost: 0.2746186252201084\n",
      "Cost: 0.27461870522010823\n",
      "Cost: 0.2589297036269217\n",
      "Cost: 0.25892972362691424\n",
      "Cost: 0.25892988362691616\n",
      "Cost: 0.25892990362691537\n",
      "Cost: 0.25892982362691297\n",
      "Cost: 0.25892990362691426\n",
      "Cost: 0.23483681579074855\n",
      "Cost: 0.23483679579074512\n",
      "Cost: 0.23483675579074537\n",
      "Cost: 0.23483673579074793\n",
      "Cost: 0.2348368757907464\n",
      "Cost: 0.23483669579074706\n",
      "Cost: 0.21090339019759202\n",
      "Cost: 0.21090342019759717\n",
      "Cost: 0.21090336019759576\n",
      "Cost: 0.2109035001975949\n",
      "Cost: 0.21090346019759182\n",
      "Cost: 0.21090338019758986\n",
      "Cost: 0.21090336019759276\n",
      "Cost: 0.245693483353277\n",
      "Cost: 0.24569350335327556\n",
      "Cost: 0.2456933033532781\n",
      "Cost: 0.24569364335327915\n",
      "Cost: 0.24569356335327786\n",
      "Cost: 0.24569344335327725\n",
      "Cost: 0.20244295889062058\n",
      "Cost: 0.2024427788906168\n",
      "Cost: 0.20244287889061707\n",
      "Cost: 0.20244303889062276\n",
      "Cost: 0.20244289889061406\n",
      "Cost: 0.2024429188906195\n",
      "Cost: 0.18995870788576663\n",
      "Cost: 0.1899587178857688\n",
      "Cost: 0.18995865788576782\n",
      "Cost: 0.1899586778857686\n",
      "Cost: 0.18995879788576608\n",
      "Cost: 0.18995867788576792\n",
      "Cost: 0.18995859788576674\n",
      "Cost: 0.18955226048519008\n",
      "Cost: 0.18955244048518766\n",
      "Cost: 0.18955242048518978\n",
      "Cost: 0.18955228048518774\n",
      "Cost: 0.18955234048519204\n",
      "Cost: 0.18955242048519255\n",
      "Cost: 0.1747284461809695\n",
      "Cost: 0.17472842618097095\n",
      "Cost: 0.17472850618097135\n",
      "Cost: 0.17472828618097136\n",
      "Cost: 0.17472838618097142\n",
      "Cost: 0.1747284261809654\n",
      "Cost: 0.1683850356098457\n",
      "Cost: 0.16838512560985047\n",
      "Cost: 0.16838518560984744\n",
      "Cost: 0.16838506560984728\n",
      "Cost: 0.1683851056098466\n",
      "Cost: 0.16838502560984708\n",
      "Cost: 0.16838502560984586\n",
      "Cost: 0.16014435803836297\n",
      "Cost: 0.16014427803836478\n",
      "Cost: 0.16014415803836063\n",
      "Cost: 0.16014427803836012\n",
      "Cost: 0.16014431803836277\n",
      "Cost: 0.16014417803836328\n",
      "Cost: 0.1740360954513776\n",
      "Cost: 0.17403603545138285\n",
      "Cost: 0.17403611545138148\n",
      "Cost: 0.1740361754513769\n",
      "Cost: 0.1740360754513786\n",
      "Cost: 0.17403595545138023\n",
      "Cost: 0.15631271460775115\n",
      "Cost: 0.15631274460775052\n",
      "Cost: 0.15631264460775024\n",
      "Cost: 0.1563128246077496\n",
      "Cost: 0.15631280460775104\n",
      "Cost: 0.15631280460774993\n",
      "Cost: 0.15631252460775064\n",
      "Cost: 0.1474397209786274\n",
      "Cost: 0.14743960097862302\n",
      "Cost: 0.14743968097862253\n",
      "Cost: 0.14743976097862338\n",
      "Cost: 0.14743988097862376\n",
      "Cost: 0.14743970097862474\n",
      "Cost: 0.14437409961584824\n",
      "Cost: 0.1443741796158482\n",
      "Cost: 0.144374119615849\n",
      "Cost: 0.14437415961584343\n",
      "Cost: 0.14437403961584638\n",
      "Cost: 0.14437413961584467\n",
      "Cost: 0.1400868033158888\n",
      "Cost: 0.1400867933158893\n",
      "Cost: 0.14008687331589237\n",
      "Cost: 0.14008689331588736\n",
      "Cost: 0.1400868533158927\n",
      "Cost: 0.14008681331588763\n",
      "Cost: 0.14008683331589006\n",
      "Cost: 0.13874151886148484\n",
      "Cost: 0.13874133886148415\n",
      "Cost: 0.13874149886148363\n",
      "Cost: 0.13874135886148314\n",
      "Cost: 0.1387414388614842\n",
      "Cost: 0.13874133886148604\n",
      "Cost: 0.1358795707906465\n",
      "Cost: 0.13587949079064654\n",
      "Cost: 0.1358795907906477\n",
      "Cost: 0.13587961079064892\n",
      "Cost: 0.13587963079064547\n",
      "Cost: 0.13587955079064373\n",
      "Cost: 0.13300209391403767\n",
      "Cost: 0.13300206391403607\n",
      "Cost: 0.13300214391403603\n",
      "Cost: 0.1330019439140359\n",
      "Cost: 0.13300198391403478\n",
      "Cost: 0.13300212391403549\n",
      "Cost: 0.133002183914039\n",
      "Cost: 0.13193954705316652\n",
      "Cost: 0.13193954705316918\n",
      "Cost: 0.13193964705316524\n",
      "Cost: 0.13193958705316605\n",
      "Cost: 0.13193980705316916\n",
      "Cost: 0.13193960705316682\n",
      "Cost: 0.12765420081988416\n",
      "Cost: 0.12765402081988392\n",
      "Cost: 0.1276541408198823\n",
      "Cost: 0.12765422081989003\n",
      "Cost: 0.12765416081988684\n",
      "Cost: 0.12765416081988762\n",
      "Cost: 0.12679130583387133\n",
      "Cost: 0.12679121583387498\n",
      "Cost: 0.12679139583386945\n",
      "Cost: 0.12679129583387405\n",
      "Cost: 0.12679139583387256\n",
      "Cost: 0.1267911758338689\n",
      "Cost: 0.12679139583387145\n",
      "Cost: 0.12222439813464721\n",
      "Cost: 0.12222437813464511\n",
      "Cost: 0.12222445813464507\n",
      "Cost: 0.12222429813464404\n",
      "Cost: 0.12222435813464545\n",
      "Cost: 0.1222242781346476\n",
      "Cost: 0.12180663358196316\n",
      "Cost: 0.12180656358196713\n",
      "Cost: 0.12180672358196616\n",
      "Cost: 0.12180664358195865\n",
      "Cost: 0.12180652358196471\n",
      "Cost: 0.12180664358196354\n",
      "Cost: 0.12180674358196193\n",
      "Cost: 0.11826622468598798\n",
      "Cost: 0.11826619468598727\n",
      "Cost: 0.11826617468598562\n",
      "Cost: 0.11826615468598574\n",
      "Cost: 0.11826623468598747\n",
      "Cost: 0.11826625468598824\n",
      "Cost: 0.11826613468598664\n",
      "Cost: 0.11826625468598741\n",
      "Cost: 0.11686134475830641\n",
      "Cost: 0.11686130475830532\n",
      "Cost: 0.1168613447583033\n",
      "Cost: 0.11686140475830338\n",
      "Cost: 0.11686128475830478\n",
      "Cost: 0.11686136475829718\n",
      "Cost: 0.11686126475829957\n",
      "Cost: 0.11483909494922546\n",
      "Cost: 0.11483923494922417\n",
      "Cost: 0.11483915494922155\n",
      "Cost: 0.11483927494921993\n",
      "Cost: 0.11483921494922318\n",
      "Cost: 0.11483909494922125\n",
      "Cost: 0.11483915494922399\n",
      "Cost: 0.11483909494922319\n",
      "Cost: 0.11365893185829952\n",
      "Cost: 0.11365893185830352\n",
      "Cost: 0.11365893185830085\n",
      "Cost: 0.11365889185830598\n",
      "Cost: 0.11365901185830525\n",
      "Cost: 0.11365891185830575\n",
      "Cost: 0.11365897185830338\n",
      "Cost: 0.1134486146847506\n",
      "Cost: 0.11344887468475369\n",
      "Cost: 0.11344883468475282\n",
      "Cost: 0.11344877468475341\n",
      "Cost: 0.11344877468475296\n",
      "Cost: 0.1134490346847616\n",
      "Cost: 0.11344891468475544\n",
      "Cost: 0.11344887468475352\n",
      "Cost: 0.11136697729930463\n",
      "Cost: 0.1113669972993125\n",
      "Cost: 0.11136687729930835\n",
      "Cost: 0.11136697729930907\n",
      "Cost: 0.11136693729930865\n",
      "Cost: 0.11136689729930956\n",
      "Cost: 0.11136695729930453\n",
      "Cost: 0.11136687729930579\n",
      "Cost: 0.1118089849805093\n",
      "Cost: 0.11180895498050859\n",
      "Cost: 0.11180915498051315\n",
      "Cost: 0.11180891498051194\n",
      "Cost: 0.11180893498051404\n",
      "Cost: 0.11180905498051154\n",
      "Cost: 0.11180897498050603\n",
      "Cost: 0.11180899498051035\n",
      "Cost: 0.11180897498050964\n",
      "Cost: 0.11022015153130411\n",
      "Cost: 0.11022013153130202\n",
      "Cost: 0.1102200115313023\n",
      "Cost: 0.11022013153129935\n",
      "Cost: 0.11022005153130339\n",
      "Cost: 0.11022009153129937\n",
      "Cost: 0.1102200915313017\n",
      "Cost: 0.11022015153130017\n",
      "Cost: 0.109826324281406\n",
      "Cost: 0.10982622428140616\n",
      "Cost: 0.10982614428140643\n",
      "Cost: 0.10982626428140348\n",
      "Cost: 0.1098262842814058\n",
      "Cost: 0.10982622428140794\n",
      "Cost: 0.10982622428140394\n",
      "Cost: 0.10982632428140461\n",
      "Cost: 0.1089858696374263\n",
      "Cost: 0.10898570963742417\n",
      "Cost: 0.10898570963742195\n",
      "Cost: 0.1089857896374239\n",
      "Cost: 0.10898584963742665\n",
      "Cost: 0.10898578963742257\n",
      "Cost: 0.1089658678309919\n",
      "Cost: 0.10896580783099004\n",
      "Cost: 0.10896584783099246\n",
      "Cost: 0.10896580783099337\n",
      "Cost: 0.10896582783099902\n",
      "Cost: 0.10896584783099335\n",
      "Cost: 0.10788103307291143\n",
      "Cost: 0.10788113307291082\n",
      "Cost: 0.10788111307290695\n",
      "Cost: 0.10788109307290862\n",
      "Cost: 0.10788115307291003\n",
      "Cost: 0.10788107307291163\n",
      "Cost: 0.10788109307291023\n",
      "Cost: 0.10748795965106227\n",
      "Cost: 0.1074878396510661\n",
      "Cost: 0.10748799965106891\n",
      "Cost: 0.10748787965106875\n",
      "Cost: 0.10748797965106548\n",
      "Cost: 0.1074879196510664\n",
      "Cost: 0.10635100275070818\n",
      "Cost: 0.10635104275070616\n",
      "Cost: 0.10635104275071282\n",
      "Cost: 0.10635102275070939\n",
      "Cost: 0.10635098275071163\n",
      "Cost: 0.10635104275070882\n",
      "Cost: 0.10573420513011743\n",
      "Cost: 0.1057341851301211\n",
      "Cost: 0.10573428513012183\n",
      "Cost: 0.10573422513011997\n",
      "Cost: 0.10573422513012108\n",
      "Cost: 0.10573430513012037\n",
      "Cost: 0.10573428513012022\n",
      "Cost: 0.10524926871482763\n",
      "Cost: 0.10524910871482816\n",
      "Cost: 0.10524910871482994\n",
      "Cost: 0.10524918871482856\n",
      "Cost: 0.1052492287148261\n",
      "Cost: 0.10524912871482527\n",
      "Cost: 0.10437155780899925\n",
      "Cost: 0.10437169780899662\n",
      "Cost: 0.10437159780899367\n",
      "Cost: 0.1043716978089984\n",
      "Cost: 0.10437167780900296\n",
      "Cost: 0.10437163780900031\n",
      "Cost: 0.10321833752969765\n",
      "Cost: 0.10321819752969762\n",
      "Cost: 0.10321819752969762\n",
      "Cost: 0.10321829752969279\n",
      "Cost: 0.10321831752969356\n",
      "Cost: 0.10321829752969412\n",
      "Cost: 0.10321829752969496\n",
      "Cost: 0.10345773908693388\n",
      "Cost: 0.10345759908693918\n",
      "Cost: 0.1034576790869338\n",
      "Cost: 0.10345765908693393\n",
      "Cost: 0.10345765908693548\n",
      "Cost: 0.10345767908693503\n",
      "Cost: 0.10262369260294306\n",
      "Cost: 0.10262363260294476\n",
      "Cost: 0.10262371260294649\n",
      "Cost: 0.10262377260294124\n",
      "Cost: 0.10262361260293866\n",
      "Cost: 0.10262363260294076\n",
      "Cost: 0.10244359577420349\n",
      "Cost: 0.10244365577420401\n",
      "Cost: 0.10244353577420208\n",
      "Cost: 0.10244367577420078\n",
      "Cost: 0.10244353577419453\n",
      "Cost: 0.10244363577420036\n",
      "Cost: 0.1024435557741989\n",
      "Cost: 0.10108542228322061\n",
      "Cost: 0.1010854022832225\n",
      "Cost: 0.10108540228321695\n",
      "Cost: 0.10108542228322438\n",
      "Cost: 0.10108546228322147\n",
      "Cost: 0.10108540228322217\n",
      "Cost: 0.10235316134936691\n",
      "Cost: 0.10235310134936594\n",
      "Cost: 0.10235298134936534\n",
      "Cost: 0.10235304134936986\n",
      "Cost: 0.1023530413493672\n",
      "Cost: 0.10235294134936647\n",
      "Cost: 0.10047354358015732\n",
      "Cost: 0.1004736835801578\n",
      "Cost: 0.10047352358016144\n",
      "Cost: 0.10047346358016003\n",
      "Cost: 0.10047348358015791\n",
      "Cost: 0.10047362358015773\n",
      "Cost: 0.10047362358015756\n",
      "Cost: 0.09959268810622339\n",
      "Cost: 0.09959290810622339\n",
      "Cost: 0.09959294810623158\n",
      "Cost: 0.09959280810622755\n",
      "Cost: 0.09959280810622444\n",
      "Cost: 0.09959286810622396\n",
      "Cost: 0.09880534636860398\n",
      "Cost: 0.0988053663686043\n",
      "Cost: 0.09880544636860071\n",
      "Cost: 0.09880538636859841\n",
      "Cost: 0.09880538636860463\n",
      "Cost: 0.0988052863686028\n",
      "Cost: 0.09767842443491159\n",
      "Cost: 0.09767832443490998\n",
      "Cost: 0.09767846443491224\n",
      "Cost: 0.09767844443491325\n",
      "Cost: 0.09767848443491323\n",
      "Cost: 0.09767836443491063\n",
      "Cost: 0.09767834443491269\n",
      "Cost: 0.09719175173002725\n",
      "Cost: 0.09719185173002398\n",
      "Cost: 0.09719187173002453\n",
      "Cost: 0.09719173173002538\n",
      "Cost: 0.09719183173002788\n",
      "Cost: 0.0971918317300261\n",
      "Cost: 0.09567694840741331\n",
      "Cost: 0.09567694840741509\n",
      "Cost: 0.09567708840741335\n",
      "Cost: 0.09567708840741823\n",
      "Cost: 0.09567696840742052\n",
      "Cost: 0.09567716840741974\n",
      "Cost: 0.09334189948209742\n",
      "Cost: 0.09334193948209496\n",
      "Cost: 0.0933419194820913\n",
      "Cost: 0.09334207948209522\n",
      "Cost: 0.09334193948209474\n",
      "Cost: 0.09334189948209254\n",
      "Cost: 0.0933420394820923\n",
      "Cost: 0.09082604031745006\n",
      "Cost: 0.09082588031744926\n",
      "Cost: 0.09082594031744934\n",
      "Cost: 0.09082574031745277\n",
      "Cost: 0.09082590031745491\n",
      "Cost: 0.09082570031744402\n",
      "Cost: 0.09257128935574097\n",
      "Cost: 0.09257126935573931\n",
      "Cost: 0.09257122935574355\n",
      "Cost: 0.09257124935574121\n",
      "Cost: 0.0925712693557453\n",
      "Cost: 0.09257124935574232\n",
      "Cost: 0.08838627983870015\n",
      "Cost: 0.08838635983870455\n",
      "Cost: 0.08838633983870911\n",
      "Cost: 0.08838643983871072\n",
      "Cost: 0.0883864198387124\n",
      "Cost: 0.0883865398387079\n",
      "Cost: 0.08838637983871003\n",
      "Cost: 0.08620770195054973\n",
      "Cost: 0.08620760195055034\n",
      "Cost: 0.08620758195055268\n",
      "Cost: 0.08620754195054937\n",
      "Cost: 0.08620760195054901\n",
      "Cost: 0.08620764195054832\n",
      "Cost: 0.08420633910680171\n",
      "Cost: 0.08420633910680438\n",
      "Cost: 0.08420641910680567\n",
      "Cost: 0.08420639910680601\n",
      "Cost: 0.08420629910680506\n",
      "Cost: 0.08420643910680443\n",
      "Cost: 0.08251210353913313\n",
      "Cost: 0.08251204353913083\n",
      "Cost: 0.0825120435391335\n",
      "Cost: 0.08251200353912885\n",
      "Cost: 0.08251198353913386\n",
      "Cost: 0.08251198353913008\n",
      "Cost: 0.08251194353912944\n",
      "Cost: 0.07890135343810396\n",
      "Cost: 0.07890123343810514\n",
      "Cost: 0.07890129343810211\n",
      "Cost: 0.07890133343810075\n",
      "Cost: 0.07890141343810138\n",
      "Cost: 0.07890131343809954\n",
      "Cost: 0.07890139343810161\n",
      "Cost: 0.0779565683193801\n",
      "Cost: 0.07795667831938298\n",
      "Cost: 0.07795659831938258\n",
      "Cost: 0.07795657831938203\n",
      "Cost: 0.07795657831938292\n",
      "Cost: 0.0779566183193778\n",
      "Cost: 0.07795657831938148\n",
      "Cost: 0.07795651831937979\n",
      "Cost: 0.07680974378594815\n",
      "Cost: 0.07680970378595017\n",
      "Cost: 0.07680968378595185\n",
      "Cost: 0.0768097437859517\n",
      "Cost: 0.07680964378594765\n",
      "Cost: 0.07680970378594573\n",
      "Cost: 0.07680970378594756\n",
      "Cost: 0.07661335211656946\n",
      "Cost: 0.07661335211656502\n",
      "Cost: 0.07661341211656554\n",
      "Cost: 0.07661345211656286\n",
      "Cost: 0.07661331211656681\n",
      "Cost: 0.07661337211656623\n",
      "Cost: 0.07661335211656646\n",
      "Cost: 0.07551284381750199\n",
      "Cost: 0.07551294381749739\n",
      "Cost: 0.07551292381749884\n",
      "Cost: 0.07551294381749606\n",
      "Cost: 0.07551278381749392\n",
      "Cost: 0.07551286381749588\n",
      "Cost: 0.07551284381749655\n",
      "Cost: 0.07671864731166164\n",
      "Cost: 0.07671856731165724\n",
      "Cost: 0.07671854731165426\n",
      "Cost: 0.07671868731165785\n",
      "Cost: 0.07671856731166013\n",
      "Cost: 0.07671864731165698\n",
      "Cost: 0.07671860731165533\n",
      "Cost: 0.07493794601965827\n",
      "Cost: 0.07493810601965818\n",
      "Cost: 0.07493798601965891\n",
      "Cost: 0.07493788601965952\n",
      "Cost: 0.07493806601965665\n",
      "Cost: 0.07493798601965891\n",
      "Cost: 0.0749380460196581\n",
      "Cost: 0.07442518943818\n",
      "Cost: 0.07442526943817818\n",
      "Cost: 0.07442520943818076\n",
      "Cost: 0.07442522943817709\n",
      "Cost: 0.0744250894381786\n",
      "Cost: 0.07442522943818064\n",
      "Cost: 0.07442524943817974\n",
      "Cost: 0.07393437873076272\n",
      "Cost: 0.07393435873076462\n",
      "Cost: 0.07393433873075853\n",
      "Cost: 0.07393427873075933\n",
      "Cost: 0.07393431873075776\n",
      "Cost: 0.07393431873076264\n",
      "Cost: 0.07393431873076187\n",
      "Cost: 0.07378072101070132\n",
      "Cost: 0.07378073101069815\n",
      "Cost: 0.07378073101069949\n",
      "Cost: 0.07378075101069848\n",
      "Cost: 0.0737806910106964\n",
      "Cost: 0.07378065101069886\n",
      "Cost: 0.07378073101070126\n",
      "Cost: 0.0737806510107018\n",
      "Cost: 0.07349107536029156\n",
      "Cost: 0.07349117536029273\n",
      "Cost: 0.07349099536029294\n",
      "Cost: 0.07349115536029019\n",
      "Cost: 0.07349113536028898\n",
      "Cost: 0.0734911353602922\n",
      "Cost: 0.07349101536029298\n",
      "Cost: 0.07342292355013871\n",
      "Cost: 0.07342312355013572\n",
      "Cost: 0.07342282355013932\n",
      "Cost: 0.07342286355013597\n",
      "Cost: 0.07342292355013738\n",
      "Cost: 0.07342290355013817\n",
      "Cost: 0.07342284355013864\n",
      "Cost: 0.07295989940344533\n",
      "Cost: 0.07295987940344634\n",
      "Cost: 0.07295993940344442\n",
      "Cost: 0.0729599194034441\n",
      "Cost: 0.07295991940344698\n",
      "Cost: 0.07295983940344802\n",
      "Cost: 0.07295981940344781\n",
      "Cost: 0.07295983940344714\n",
      "Cost: 0.07365538586074348\n",
      "Cost: 0.0736552058607388\n",
      "Cost: 0.07365558586074804\n",
      "Cost: 0.07365538586074459\n",
      "Cost: 0.07365540586073825\n",
      "Cost: 0.0736553858607406\n",
      "Cost: 0.07275803881587788\n",
      "Cost: 0.0727580288158766\n",
      "Cost: 0.07275794881587798\n",
      "Cost: 0.07275802881587883\n",
      "Cost: 0.07275796881587875\n",
      "Cost: 0.07275798881587862\n",
      "Cost: 0.07275804881587893\n",
      "Cost: 0.07263462036517282\n",
      "Cost: 0.07263467036517118\n",
      "Cost: 0.07263463036517231\n",
      "Cost: 0.07263456387429218\n",
      "Cost: 0.07263462387429448\n",
      "Cost: 0.0726346238742967\n",
      "Cost: 0.07263467036517507\n",
      "Cost: 0.0726596979740407\n",
      "Cost: 0.0726597779740411\n",
      "Cost: 0.07265983797404274\n",
      "Cost: 0.072659777974044\n",
      "Cost: 0.0726596979740376\n",
      "Cost: 0.07265987797404017\n",
      "Cost: 0.07265981797404114\n",
      "Cost: 0.07260497322646381\n",
      "Cost: 0.07260507322646187\n",
      "Cost: 0.07260487322646131\n",
      "Cost: 0.07260499322646013\n",
      "Cost: 0.07260493322645917\n",
      "Cost: 0.07260503322646211\n",
      "Cost: 0.07260501322646062\n",
      "Cost: 0.07268911591358512\n",
      "Cost: 0.07268917591359098\n",
      "Cost: 0.0726892159135894\n",
      "Cost: 0.07268921591358585\n",
      "Cost: 0.07268919591358397\n",
      "Cost: 0.07268905591358593\n",
      "Cost: 0.07268911591358429\n",
      "Cost: 0.072553704528529\n",
      "Cost: 0.07255370452853077\n",
      "Cost: 0.07255374452852809\n",
      "Cost: 0.07255376452853018\n",
      "Cost: 0.07255374452852653\n",
      "Cost: 0.0725536445285287\n",
      "Cost: 0.07250196196583318\n",
      "Cost: 0.0725020819658329\n",
      "Cost: 0.07250194196583597\n",
      "Cost: 0.07250208196583556\n",
      "Cost: 0.07250200196583716\n",
      "Cost: 0.07250210196583899\n",
      "Cost: 0.07250196196583923\n",
      "Cost: 0.07251245038916099\n",
      "Cost: 0.07251253038916272\n",
      "Cost: 0.07251257038916559\n",
      "Cost: 0.07251253038916516\n",
      "Cost: 0.07251249038916341\n",
      "Cost: 0.0725124903891653\n",
      "Cost: 0.07323529689015333\n",
      "Cost: 0.07323523689015059\n",
      "Cost: 0.07323511689015176\n",
      "Cost: 0.07323533689015398\n",
      "Cost: 0.07323523689015325\n",
      "Cost: 0.0732351368901532\n",
      "Cost: 0.07323523689015186\n",
      "Cost: 0.07247503139866313\n",
      "Cost: 0.07247491139866186\n",
      "Cost: 0.07247493139866662\n",
      "Cost: 0.07247491139866385\n",
      "Cost: 0.07247503139866646\n",
      "Cost: 0.07247501139866525\n",
      "Cost: 0.07247499139866237\n",
      "Cost: 0.07248680710029691\n",
      "Cost: 0.07248688710030353\n",
      "Cost: 0.07248692710030417\n",
      "Cost: 0.072486827100303\n",
      "Cost: 0.07248686710030432\n",
      "Cost: 0.07248682710030745\n",
      "Cost: 0.07248686710030476\n",
      "Cost: 0.07246693280473819\n",
      "Cost: 0.07246691280473565\n",
      "Cost: 0.07246693280474263\n",
      "Cost: 0.07246683280474102\n",
      "Cost: 0.07246695280474096\n",
      "Cost: 0.07246685280474223\n",
      "Cost: 0.07246685280474295\n",
      "Cost: 0.07241682648055324\n",
      "Cost: 0.07241678648055482\n",
      "Cost: 0.07241676648054961\n",
      "Cost: 0.07241688648055532\n",
      "Cost: 0.07241686648055366\n",
      "Cost: 0.07241686648055433\n",
      "Cost: 0.07242148482098668\n",
      "Cost: 0.0724215648209924\n",
      "Cost: 0.0724215648209924\n",
      "Cost: 0.072421504820993\n",
      "Cost: 0.07242148482099045\n",
      "Cost: 0.07242150482099721\n",
      "Cost: 0.0724215248209942\n",
      "Cost: 0.07242154482099569\n",
      "Cost: 0.07240303319960595\n",
      "Cost: 0.0724030731996066\n",
      "Cost: 0.07240305319960672\n",
      "Cost: 0.07240301319960318\n",
      "Cost: 0.07240301319960452\n",
      "Cost: 0.07240299319960808\n",
      "Cost: 0.07240307319960715\n",
      "Cost: 0.0724030131996069\n",
      "Cost: 0.07232906501724458\n",
      "Cost: 0.07232906501724636\n",
      "Cost: 0.07232912501724777\n",
      "Cost: 0.07232910501724833\n",
      "Cost: 0.07232910501724789\n",
      "Cost: 0.07232910501724645\n",
      "Cost: 0.07229345933250433\n",
      "Cost: 0.07229349933250365\n",
      "Cost: 0.07229331933250341\n",
      "Cost: 0.07229339933249981\n",
      "Cost: 0.07229347933250155\n",
      "Cost: 0.07229343933250557\n",
      "Cost: 0.07229345933250234\n",
      "Cost: 0.07217968871167016\n",
      "Cost: 0.07217962871166875\n",
      "Cost: 0.07217970871166515\n",
      "Cost: 0.07217966871166429\n",
      "Cost: 0.07217966871167183\n",
      "Cost: 0.07217968871166927\n",
      "Cost: 0.07217974871167057\n",
      "Cost: 0.07217966871166867\n",
      "Cost: 0.07208854538151561\n",
      "Cost: 0.07208846538151832\n",
      "Cost: 0.07208856538151727\n",
      "Cost: 0.07208848538151887\n",
      "Cost: 0.0720885053815163\n",
      "Cost: 0.0720885253815184\n",
      "Cost: 0.07208854538151606\n",
      "Cost: 0.07196436118754607\n",
      "Cost: 0.07196440118754538\n",
      "Cost: 0.07196420118754793\n",
      "Cost: 0.07196426118754756\n",
      "Cost: 0.07196418118754605\n",
      "Cost: 0.07196436118754207\n",
      "Cost: 0.07196430118754576\n",
      "Cost: 0.07196428118754644\n",
      "Cost: 0.07170410687066711\n",
      "Cost: 0.07170416687066852\n",
      "Cost: 0.07170406687066314\n",
      "Cost: 0.07170406687066269\n",
      "Cost: 0.07170400687066195\n",
      "Cost: 0.07170410687066645\n",
      "Cost: 0.07170412687066777\n",
      "Cost: 0.07139274248887842\n",
      "Cost: 0.07139268248888056\n",
      "Cost: 0.07139264248887858\n",
      "Cost: 0.07139272248887765\n",
      "Cost: 0.0713926624888769\n",
      "Cost: 0.07139278248887729\n",
      "Cost: 0.0713928024888815\n",
      "Cost: 0.07071367069035636\n",
      "Cost: 0.07071353069035544\n",
      "Cost: 0.07071353069035854\n",
      "Cost: 0.07071359069035241\n",
      "Cost: 0.07071339069035185\n",
      "Cost: 0.07071357069035697\n",
      "Cost: 0.0707135706903523\n",
      "Cost: 0.07036217414461321\n",
      "Cost: 0.07036219414461442\n",
      "Cost: 0.07036215414461289\n",
      "Cost: 0.07036235414461256\n",
      "Cost: 0.07036225414461716\n",
      "Cost: 0.07036227414461815\n",
      "Cost: 0.07036217414461454\n",
      "Cost: 0.07036217414461532\n",
      "Cost: 0.06987519114421048\n",
      "Cost: 0.06987530114420937\n",
      "Cost: 0.06987508114421337\n",
      "Cost: 0.06987522114420852\n",
      "Cost: 0.06987512114420802\n",
      "Cost: 0.06987520114421131\n",
      "Cost: 0.06987516114420944\n",
      "Cost: 0.06911376948310584\n",
      "Cost: 0.06911384948310757\n",
      "Cost: 0.0691136894831081\n",
      "Cost: 0.06911374948310707\n",
      "Cost: 0.06911364948310346\n",
      "Cost: 0.0691137694831034\n",
      "Cost: 0.0691137094831062\n",
      "Cost: 0.06858603294004746\n",
      "Cost: 0.06858595294004173\n",
      "Cost: 0.06858599294004593\n",
      "Cost: 0.0685860129400498\n",
      "Cost: 0.06858603294004724\n",
      "Cost: 0.06858603294004724\n",
      "Cost: 0.0685859929400456\n",
      "Cost: 0.07143620481964016\n",
      "Cost: 0.07143602481963415\n",
      "Cost: 0.0714359648196323\n",
      "Cost: 0.07143604481963581\n",
      "Cost: 0.07143598481964195\n",
      "Cost: 0.07143610481963955\n",
      "Cost: 0.06853959286671155\n",
      "Cost: 0.06853964286671124\n",
      "Cost: 0.06853962286671225\n",
      "Cost: 0.06853960286671149\n",
      "Cost: 0.068539682866713\n",
      "Cost: 0.06853958286671005\n",
      "Cost: 0.06853966286671145\n",
      "Cost: 0.06838907245846129\n",
      "Cost: 0.068388992458464\n",
      "Cost: 0.06838911245846371\n",
      "Cost: 0.06838899245845778\n",
      "Cost: 0.06838899245845867\n",
      "Cost: 0.0683889324584617\n",
      "Cost: 0.0683889724584579\n",
      "Cost: 0.06841982000528352\n",
      "Cost: 0.0684198200052813\n",
      "Cost: 0.06841998000528388\n",
      "Cost: 0.06841988000528093\n",
      "Cost: 0.06841986000528238\n",
      "Cost: 0.06841982000527752\n",
      "Cost: 0.06841980000527453\n",
      "Cost: 0.06822919826677623\n",
      "Cost: 0.06822921826677922\n",
      "Cost: 0.06822909826677728\n",
      "Cost: 0.06822915826677514\n",
      "Cost: 0.06822915826677381\n",
      "Cost: 0.0682291582667737\n",
      "Cost: 0.06822911826677278\n",
      "Cost: 0.06795814665031505\n",
      "Cost: 0.06795810665031662\n",
      "Cost: 0.06795804665031432\n",
      "Cost: 0.06795804665031388\n",
      "Cost: 0.06795806665031243\n",
      "Cost: 0.06795806665031398\n",
      "Cost: 0.06795802665031467\n",
      "Cost: 0.06800358428742506\n",
      "Cost: 0.06800364428742514\n",
      "Cost: 0.06800366428742458\n",
      "Cost: 0.06800372428742732\n",
      "Cost: 0.06800374428742587\n",
      "Cost: 0.0680035842874245\n",
      "Cost: 0.06800366428742269\n",
      "Cost: 0.06790965641069066\n",
      "Cost: 0.0679097364106915\n",
      "Cost: 0.06790969641068775\n",
      "Cost: 0.06790961641069135\n",
      "Cost: 0.06790955641068794\n",
      "Cost: 0.06790963641069211\n",
      "Cost: 0.06790957641068932\n",
      "Cost: 0.0677668063848698\n",
      "Cost: 0.06776676638487049\n",
      "Cost: 0.06776672638486585\n",
      "Cost: 0.06776680638486914\n",
      "Cost: 0.0677668263848679\n",
      "Cost: 0.06776672638486851\n",
      "Cost: 0.06776688638486743\n",
      "Cost: 0.0677349950363919\n",
      "Cost: 0.06773503503639565\n",
      "Cost: 0.06773501503639488\n",
      "Cost: 0.06773505503639264\n",
      "Cost: 0.06773499503639101\n",
      "Cost: 0.06773493503639315\n",
      "Cost: 0.0677349750363949\n",
      "Cost: 0.06764350417731983\n",
      "Cost: 0.06764346417732052\n",
      "Cost: 0.06764354417731915\n",
      "Cost: 0.06764362417732044\n",
      "Cost: 0.06764348417732195\n",
      "Cost: 0.06764356417731958\n",
      "Cost: 0.06764356417732102\n",
      "Cost: 0.0676763671192426\n",
      "Cost: 0.06767638711923804\n",
      "Cost: 0.06767648711923743\n",
      "Cost: 0.067676447119241\n",
      "Cost: 0.0676764271192398\n",
      "Cost: 0.06767644711923657\n",
      "Cost: 0.06767640711924047\n",
      "Cost: 0.06761386991619223\n",
      "Cost: 0.06761382991619336\n",
      "Cost: 0.0676139499161944\n",
      "Cost: 0.067613869916194\n",
      "Cost: 0.0676138899161941\n",
      "Cost: 0.06761386991619578\n",
      "Cost: 0.06761388991619921\n",
      "Cost: 0.06755845343821135\n",
      "Cost: 0.06755839343821171\n",
      "Cost: 0.06755847343820945\n",
      "Cost: 0.06755843343821014\n",
      "Cost: 0.06755849343821288\n",
      "Cost: 0.06755833343821363\n",
      "Cost: 0.0675584534382119\n",
      "Cost: 0.06755849343821138\n",
      "Cost: 0.06749217458738507\n",
      "Cost: 0.0674920345873877\n",
      "Cost: 0.06749211458738788\n",
      "Cost: 0.06749213458738287\n",
      "Cost: 0.06749201458738294\n",
      "Cost: 0.06749201458738582\n",
      "Cost: 0.06742207272730014\n",
      "Cost: 0.06742200272730278\n",
      "Cost: 0.06742204272730254\n",
      "Cost: 0.06742206272730508\n",
      "Cost: 0.0674221027273024\n",
      "Cost: 0.06742204272730032\n",
      "Cost: 0.06742204272730165\n",
      "Cost: 0.06740385380669331\n",
      "Cost: 0.06740377380669468\n",
      "Cost: 0.06740377380669513\n",
      "Cost: 0.06740385380669597\n",
      "Cost: 0.06740387380669186\n",
      "Cost: 0.06740383380669265\n",
      "Cost: 0.06740385380669336\n",
      "Cost: 0.06733543137330465\n",
      "Cost: 0.06733545137330586\n",
      "Cost: 0.06733553137330493\n",
      "Cost: 0.0673354713733093\n",
      "Cost: 0.0673354713733053\n",
      "Cost: 0.06733549137330462\n",
      "Cost: 0.06730879011571561\n",
      "Cost: 0.06730873011571731\n",
      "Cost: 0.06730887011571779\n",
      "Cost: 0.06730883011571603\n",
      "Cost: 0.06730885011571902\n",
      "Cost: 0.0673087701157144\n",
      "Cost: 0.06724080085977702\n",
      "Cost: 0.06724072085977528\n",
      "Cost: 0.06724080085977369\n",
      "Cost: 0.06724072085977106\n",
      "Cost: 0.06724082085977867\n",
      "Cost: 0.06724066085977864\n",
      "Cost: 0.06724072085978022\n",
      "Cost: 0.067219283483101\n",
      "Cost: 0.06721910348309809\n",
      "Cost: 0.06721914348309807\n",
      "Cost: 0.06721908348309732\n",
      "Cost: 0.0672192434830999\n",
      "Cost: 0.06721922348310158\n",
      "Cost: 0.06715408293784215\n",
      "Cost: 0.06715406293784361\n",
      "Cost: 0.06715400293783998\n",
      "Cost: 0.06715408293784127\n",
      "Cost: 0.06715398293783965\n",
      "Cost: 0.0671541029378407\n",
      "Cost: 0.0671204766518206\n",
      "Cost: 0.0671204366518204\n",
      "Cost: 0.06712051665182257\n",
      "Cost: 0.06712047665182548\n",
      "Cost: 0.06712053665182312\n",
      "Cost: 0.06712029665182813\n",
      "Cost: 0.06712055665182694\n",
      "Cost: 0.06701998791313903\n",
      "Cost: 0.06701996791313826\n",
      "Cost: 0.06702008791314064\n",
      "Cost: 0.06702002791314234\n",
      "Cost: 0.06702006791313676\n",
      "Cost: 0.06702004791314144\n",
      "Cost: 0.0668826111052593\n",
      "Cost: 0.06688251110525946\n",
      "Cost: 0.06688249110526136\n",
      "Cost: 0.06688257110525932\n",
      "Cost: 0.06688247110525859\n",
      "Cost: 0.0668825911052582\n",
      "Cost: 0.06677349238064077\n",
      "Cost: 0.06677343238064469\n",
      "Cost: 0.06677337238064417\n",
      "Cost: 0.0667734123806386\n",
      "Cost: 0.06677341238064081\n",
      "Cost: 0.06677335238064251\n",
      "Cost: 0.06677339238064471\n",
      "Cost: 0.066773372380643\n",
      "Cost: 0.06659498944753549\n",
      "Cost: 0.0665949894475306\n",
      "Cost: 0.06659494944753573\n",
      "Cost: 0.06659492944753563\n",
      "Cost: 0.0665950094475327\n",
      "Cost: 0.06659494944753218\n",
      "Cost: 0.06633912641385141\n",
      "Cost: 0.06633922641384903\n",
      "Cost: 0.06633910641384709\n",
      "Cost: 0.06633928641384755\n",
      "Cost: 0.066339286413848\n",
      "Cost: 0.0663393264138462\n",
      "Cost: 0.07744183871954864\n",
      "Cost: 0.07744193871955247\n",
      "Cost: 0.0774418587195514\n",
      "Cost: 0.07744181871955143\n",
      "Cost: 0.0774418787195484\n",
      "Cost: 0.07744195871954947\n",
      "Cost: 0.07744185871954808\n",
      "Cost: 0.06592486192356363\n",
      "Cost: 0.06592490192356339\n",
      "Cost: 0.06592484192356242\n",
      "Cost: 0.0659250419235632\n",
      "Cost: 0.06592496192356258\n",
      "Cost: 0.06592512192356471\n",
      "Cost: 0.06577576849903106\n",
      "Cost: 0.06577566849903034\n",
      "Cost: 0.06577560849903426\n",
      "Cost: 0.06577580849903127\n",
      "Cost: 0.0657756284990348\n",
      "Cost: 0.06577566849903423\n",
      "Cost: 0.0657757284990337\n",
      "Cost: 0.06515129908939915\n",
      "Cost: 0.06515153908939902\n",
      "Cost: 0.06515141908939442\n",
      "Cost: 0.06515127908939727\n",
      "Cost: 0.06515131908939503\n",
      "Cost: 0.06515133908939569\n",
      "Cost: 0.0650654283569384\n",
      "Cost: 0.06506542835694062\n",
      "Cost: 0.0650655283569409\n",
      "Cost: 0.0650655883569412\n",
      "Cost: 0.06506544835694805\n",
      "Cost: 0.06506536835694066\n",
      "Cost: 0.06492328682013196\n",
      "Cost: 0.06492317682012597\n",
      "Cost: 0.06492325682012814\n",
      "Cost: 0.06492343682012594\n",
      "Cost: 0.06492325682013037\n",
      "Cost: 0.06492329682013301\n",
      "Cost: 0.06492317682013352\n",
      "Cost: 0.06294495684683486\n",
      "Cost: 0.0629449768468303\n",
      "Cost: 0.06294489684682968\n",
      "Cost: 0.06294499684683195\n",
      "Cost: 0.06294491684683422\n",
      "Cost: 0.06294493684682899\n",
      "Cost: 0.06247325915408827\n",
      "Cost: 0.062473259154087385\n",
      "Cost: 0.06247327915408593\n",
      "Cost: 0.062473139154087226\n",
      "Cost: 0.06247327915408815\n",
      "Cost: 0.062473279154087596\n",
      "Cost: 0.06206406432593939\n",
      "Cost: 0.06206409432594143\n",
      "Cost: 0.06206409432593832\n",
      "Cost: 0.06206403432593913\n",
      "Cost: 0.06206421432594203\n",
      "Cost: 0.06206413432594429\n",
      "Cost: 0.06206409432593965\n",
      "Cost: 0.06275312958409873\n",
      "Cost: 0.06275302958409845\n",
      "Cost: 0.06275300958409635\n",
      "Cost: 0.06275302958409601\n",
      "Cost: 0.0627529095840943\n",
      "Cost: 0.06275302958409545\n",
      "Cost: 0.06171869389070206\n",
      "Cost: 0.06171877389069669\n",
      "Cost: 0.061718633890691765\n",
      "Cost: 0.06171877389069891\n",
      "Cost: 0.06171883389069632\n",
      "Cost: 0.0617187338906956\n",
      "Cost: 0.06171865389069636\n",
      "Cost: 0.06153602003629083\n",
      "Cost: 0.061536060036297247\n",
      "Cost: 0.06153614003629876\n",
      "Cost: 0.06153622003629605\n",
      "Cost: 0.06153612003629111\n",
      "Cost: 0.061536040036291706\n",
      "Cost: 0.06131772269227104\n",
      "Cost: 0.0613177926922675\n",
      "Cost: 0.06131771269226932\n",
      "Cost: 0.06131769269226944\n",
      "Cost: 0.061317792692269724\n",
      "Cost: 0.06131775269226375\n",
      "Cost: 0.06131767269227112\n",
      "Cost: 0.0610168166950506\n",
      "Cost: 0.061016756695054075\n",
      "Cost: 0.061016896695055445\n",
      "Cost: 0.061016836695050924\n",
      "Cost: 0.06101695669505175\n",
      "Cost: 0.06101695669505153\n",
      "Cost: 0.06057792852691117\n",
      "Cost: 0.06057789852690647\n",
      "Cost: 0.06057785852690849\n",
      "Cost: 0.06057791852691345\n",
      "Cost: 0.060577898526907134\n",
      "Cost: 0.060577938526911554\n",
      "Cost: 0.06057789852691069\n",
      "Cost: 0.060150231137657426\n",
      "Cost: 0.060150151137658356\n",
      "Cost: 0.06015025113765686\n",
      "Cost: 0.06015025113765575\n",
      "Cost: 0.060150131137659366\n",
      "Cost: 0.060150331137660815\n",
      "Cost: 0.0601501711376579\n",
      "Cost: 0.06018693298869099\n",
      "Cost: 0.060186952988693976\n",
      "Cost: 0.0601870129886945\n",
      "Cost: 0.06018691298869777\n",
      "Cost: 0.06018703298869926\n",
      "Cost: 0.06018687298869735\n",
      "Cost: 0.059925389048816946\n",
      "Cost: 0.05992534904881497\n",
      "Cost: 0.05992532904881509\n",
      "Cost: 0.059925269048818564\n",
      "Cost: 0.059925289048815555\n",
      "Cost: 0.05992532904881531\n",
      "Cost: 0.05992540904881727\n",
      "Cost: 0.05941069805040777\n",
      "Cost: 0.05941065805040491\n",
      "Cost: 0.05941077805040684\n",
      "Cost: 0.059410758050406964\n",
      "Cost: 0.05941087805040912\n",
      "Cost: 0.05941083805041247\n",
      "Cost: 0.05941075805041224\n",
      "Cost: 0.05947877367267887\n",
      "Cost: 0.05947883367267717\n",
      "Cost: 0.059478893672675254\n",
      "Cost: 0.059478993672675534\n",
      "Cost: 0.059478713672675015\n",
      "Cost: 0.05947895367267378\n",
      "Cost: 0.05926607885751134\n",
      "Cost: 0.05926605885751057\n",
      "Cost: 0.05926607885751756\n",
      "Cost: 0.059266218857517816\n",
      "Cost: 0.05926609885752143\n",
      "Cost: 0.05926609885752243\n",
      "Cost: 0.05905460377374999\n",
      "Cost: 0.05905460377375399\n",
      "Cost: 0.05905460377375399\n",
      "Cost: 0.059054503773744604\n",
      "Cost: 0.05905462377375209\n",
      "Cost: 0.05905454377375391\n",
      "Cost: 0.05905462377375448\n",
      "Cost: 0.058988155595042696\n",
      "Cost: 0.05898825559504431\n",
      "Cost: 0.05898819559504467\n",
      "Cost: 0.05898833559504671\n",
      "Cost: 0.0589880555950493\n",
      "Cost: 0.05898829559504695\n",
      "Cost: 0.05864745205646157\n",
      "Cost: 0.05864743205645858\n",
      "Cost: 0.05864743205646213\n",
      "Cost: 0.05864735205646417\n",
      "Cost: 0.05864743205646213\n",
      "Cost: 0.05864743205646258\n",
      "Cost: 0.05848252257956377\n",
      "Cost: 0.0584825225795651\n",
      "Cost: 0.05848254257956631\n",
      "Cost: 0.058482582579562736\n",
      "Cost: 0.05848250257956833\n",
      "Cost: 0.058482542579569086\n",
      "Cost: 0.05848250257956883\n",
      "Cost: 0.058235059978236636\n",
      "Cost: 0.058235119978236716\n",
      "Cost: 0.058235119978238936\n",
      "Cost: 0.05823505997823353\n",
      "Cost: 0.05823511997823716\n",
      "Cost: 0.05823517997823502\n",
      "Cost: 0.05950808968573262\n",
      "Cost: 0.05950812968573282\n",
      "Cost: 0.05950804968573819\n",
      "Cost: 0.059508249685740304\n",
      "Cost: 0.05950806968573962\n",
      "Cost: 0.05950822968573521\n",
      "Cost: 0.058021159096761525\n",
      "Cost: 0.058021149096759365\n",
      "Cost: 0.058021109096758276\n",
      "Cost: 0.058021149096761586\n",
      "Cost: 0.05802100909676266\n",
      "Cost: 0.058021029096757874\n",
      "Cost: 0.05802120909676222\n",
      "Cost: 0.05806810774105702\n",
      "Cost: 0.05806824774106128\n",
      "Cost: 0.05806802774106084\n",
      "Cost: 0.05806818774106142\n",
      "Cost: 0.0580681077410568\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m quiet\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m last_print_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m result \u001b[39m=\u001b[39m dual_annealing(\u001b[39mlambda\u001b[39;49;00m params: hungarian_cost(params, target_evals, n, ansatz_func, quiet), bounds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m hungarian_cost(result\u001b[39m.\u001b[39mx, target_evals, n, ansatz_func, quiet\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_dual_annealing.py:714\u001b[0m, in \u001b[0;36mdual_annealing\u001b[0;34m(func, bounds, args, maxiter, minimizer_kwargs, initial_temp, restart_temp_ratio, visit, accept, maxfun, seed, no_local_search, callback, x0, local_search_options)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[39m# Possible local search at the end of the strategy chain\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_local_search:\n\u001b[0;32m--> 714\u001b[0m     val \u001b[39m=\u001b[39m strategy_chain\u001b[39m.\u001b[39;49mlocal_search()\n\u001b[1;32m    715\u001b[0m     \u001b[39mif\u001b[39;00m val \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m         message\u001b[39m.\u001b[39mappend(val)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_dual_annealing.py:325\u001b[0m, in \u001b[0;36mStrategyChain.local_search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlocal_search\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    319\u001b[0m     \u001b[39m# Decision making for performing a local search\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39m# based on strategy chain results\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[39m# If energy has been improved or no improvement since too long,\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39m# performing a local search with the best strategy chain location\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menergy_state_improved:\n\u001b[1;32m    324\u001b[0m         \u001b[39m# Global energy has improved, let's see if LS improves further\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m         e, x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimizer_wrapper\u001b[39m.\u001b[39;49mlocal_search(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menergy_state\u001b[39m.\u001b[39;49mxbest,\n\u001b[1;32m    326\u001b[0m                                                    \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menergy_state\u001b[39m.\u001b[39;49mebest)\n\u001b[1;32m    327\u001b[0m         \u001b[39mif\u001b[39;00m e \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menergy_state\u001b[39m.\u001b[39mebest:\n\u001b[1;32m    328\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_improved_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_dual_annealing.py:418\u001b[0m, in \u001b[0;36mLocalSearchWrapper.local_search\u001b[0;34m(self, x, e)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlocal_search\u001b[39m(\u001b[39mself\u001b[39m, x, e):\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Run local search from the given x location where energy value is e\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     x_tmp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(x)\n\u001b[0;32m--> 418\u001b[0m     mres \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimizer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc_wrapper\u001b[39m.\u001b[39;49mfun, x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m    419\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnjev\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mres:\n\u001b[1;32m    420\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc_wrapper\u001b[39m.\u001b[39mngev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m mres\u001b[39m.\u001b[39mnjev\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_minimize.py:699\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    697\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    700\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    701\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    702\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    703\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_lbfgsb_py.py:362\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    356\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    363\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    364\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:286\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m    285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[0;32m--> 286\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_grad\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated:\n\u001b[0;32m--> 256\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad_impl()\n\u001b[1;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m approx_derivative(fun_wrapped, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, f0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf,\n\u001b[1;32m    174\u001b[0m                            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfinite_diff_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m sparsity \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[1;32m    506\u001b[0m                              use_one_sided, method)\n\u001b[1;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(sparsity) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(sparsity) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[39m=\u001b[39m x[i] \u001b[39m-\u001b[39m x0[i]  \u001b[39m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[39m=\u001b[39m fun(x) \u001b[39m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m3-point\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_1d(fun(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`fun` return value has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mmore than 1 dimension.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/optimize/_dual_annealing.py:382\u001b[0m, in \u001b[0;36mObjectiveFunWrapper.fun\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    381\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(x, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n",
      "\u001b[1;32m/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m quiet\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m last_print_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m result \u001b[39m=\u001b[39m dual_annealing(\u001b[39mlambda\u001b[39;00m params: hungarian_cost(params, target_evals, n, ansatz_func, quiet), bounds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m hungarian_cost(result\u001b[39m.\u001b[39mx, target_evals, n, ansatz_func, quiet\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhungarian_cost\u001b[39m(params, target, n, ansatz_func\u001b[39m=\u001b[39mnn_ansatz, quiet\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     cost_matrix \u001b[39m=\u001b[39m generate_cost_matrix(params, target, n, ansatz_func)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=131'>132</a>\u001b[0m     row_ind, col_ind \u001b[39m=\u001b[39m linear_sum_assignment(cost_matrix)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     min_cost \u001b[39m=\u001b[39m cost_matrix[row_ind, col_ind]\u001b[39m.\u001b[39msum()\n",
      "\u001b[1;32m/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_cost_matrix\u001b[39m(params, target, n, ansatz_func\u001b[39m=\u001b[39mnn_ansatz):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m     ansatz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdiag(ansatz_func(params, n))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     cost_function \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m a, t: \u001b[39mabs\u001b[39m(a \u001b[39m-\u001b[39m t)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m     cost_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[cost_function(a, t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m target] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m ansatz])\n",
      "\u001b[1;32m/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb Cell 8\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m \u001b[39mfor\u001b[39;00m param_count, (i,j) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sites): \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     kron_list \u001b[39m=\u001b[39m [pauli[\u001b[39m3\u001b[39m] \u001b[39mif\u001b[39;00m k \u001b[39m==\u001b[39m i \u001b[39mor\u001b[39;00m k \u001b[39m==\u001b[39m j \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39meye(\u001b[39m2\u001b[39m) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n)]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     ansatz \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m params[n \u001b[39m+\u001b[39m param_count] \u001b[39m*\u001b[39m reduce(np\u001b[39m.\u001b[39;49mkron, kron_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m ansatz \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m params[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39meye(\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alam/code/vfftn/xxx_symmetry/xxx_symmetry.ipynb#X11sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ansatz\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mkron\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/lib/shape_base.py:1179\u001b[0m, in \u001b[0;36mkron\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   1177\u001b[0m b_arr \u001b[39m=\u001b[39m expand_dims(b_arr, axis\u001b[39m=\u001b[39m\u001b[39mtuple\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, nd\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)))\n\u001b[1;32m   1178\u001b[0m \u001b[39m# In case of `mat`, convert result to `array`\u001b[39;00m\n\u001b[0;32m-> 1179\u001b[0m result \u001b[39m=\u001b[39m _nx\u001b[39m.\u001b[39;49mmultiply(a_arr, b_arr, subok\u001b[39m=\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m is_any_mat))\n\u001b[1;32m   1181\u001b[0m \u001b[39m# Reshape back\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mreshape(_nx\u001b[39m.\u001b[39mmultiply(as_, bs))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# testing Hungarian algorithm + simulated annealing with two_local ansatz\n",
    "n = 10\n",
    "j = 2\n",
    "m = 2   \n",
    "\n",
    "Sz = total_sz(n)\n",
    "S_sq = total_spin_squared(n)\n",
    "P, basis = projector(n,j,m)\n",
    "\n",
    "print(basis.shape[1]) \n",
    "eff_ham = basis.conj().T @ xxx_ham(n) @ basis\n",
    "target_evals = np.linalg.eigvalsh(eff_ham)\n",
    "\n",
    "ansatz_func = two_local_ansatz\n",
    "bounds = [(-1, 1)] * (n + 1 + int(n*(n-1)/2))  # need to be expanded\n",
    "print(len(bounds))\n",
    "quiet=True\n",
    "\n",
    "last_print_time = time.time()\n",
    "result = dual_annealing(lambda params: hungarian_cost(params, target_evals, n, ansatz_func, quiet), bounds)\n",
    "\n",
    "hungarian_cost(result.x, target_evals, n, ansatz_func, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# testing Hungarian algorithm + simulated annealing with two_local ansatz on full XXX Hamiltonian\n",
    "n = 4\n",
    "\n",
    "target_evals = np.linalg.eigvalsh(xxx_ham(n))\n",
    "print(len(target_evals))\n",
    "\n",
    "ansatz_func = two_local_ansatz\n",
    "bounds = [(-2, 2)] * (n + 1 + int(n*(n-1)/2))  # need to be expanded\n",
    "#bounds = [(-2,2)] * (n+1)\n",
    "print(len(bounds))\n",
    "quiet=True\n",
    "\n",
    "last_print_time = time.time()\n",
    "#result = dual_annealing(lambda params: hungarian_cost(params, target_evals, n, ansatz_func, quiet), bounds)\n",
    "\n",
    "#hungarian_cost(result.x, target_evals, n, ansatz_func, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6160254 , -0.95710678, -0.95710678, -0.95710678, -0.25      ,\n",
       "       -0.25      , -0.25      ,  0.1160254 ,  0.45710678,  0.45710678,\n",
       "        0.45710678,  0.75      ,  0.75      ,  0.75      ,  0.75      ,\n",
       "        0.75      ])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_evals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.92788625 -1.20710678 -0.66081859  0.20710678  0.58870484]\n"
     ]
    }
   ],
   "source": [
    "# testing Hungarian algorithm + genetic algorithm with two_local ansatz\n",
    "n = 5\n",
    "j = 1/2\n",
    "m = 1/2   \n",
    "\n",
    "Sz = total_sz(n)\n",
    "S_sq = total_spin_squared(n)\n",
    "P, basis = projector(n,j,m)\n",
    "\n",
    "eff_ham = basis.conj().T @ xxx_ham(n) @ basis\n",
    "target_evals = np.linalg.eigvalsh(eff_ham)\n",
    "print(target_evals) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.6160254  -0.95710678 -0.95710678 -0.95710678 -0.25       -0.25\n",
      " -0.25        0.1160254   0.45710678  0.45710678  0.45710678  0.75\n",
      "  0.75        0.75        0.75        0.75      ]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "target_evals = np.linalg.eigvalsh(xxx_ham(n))\n",
    "print(target_evals)\n",
    "\n",
    "-0.25, 0.75\n",
    "-0.95710678, 0.45710678\n",
    "1.6160254, 0.1160254\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.462172   -3.06155281 -1.5962912  -1.5962912  -1.         -0.65138782\n",
      " -0.65138782 -0.60753298  1.06155281  1.06970498  1.0962912   1.0962912\n",
      "  1.15138782  1.15138782  3.          3.        ]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "target_evals = np.linalg.eigvalsh(xxz_ham(n))\n",
    "print(target_evals)\n",
    "\n",
    "-3.462172\n",
    "-3.06155281, 1.06155281\n",
    "-1, 3\n",
    "-0.65138782, 1.15138782, -1.5962912, 1.0962912\n",
    "-0.60753298, 1.06970498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.47213595 -3.23606798 -3.23606798 -2.         -1.23606798 -1.23606798\n",
      "  0.          0.          0.          0.          1.23606798  1.23606798\n",
      "  2.          3.23606798  3.23606798  4.47213595]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 4.99957942e-09, -2.50021026e-09, -2.50021026e-09, -4.44089210e-16,\n",
       "       -2.50021026e-09, -2.50021026e-09,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.50021026e-09,  2.50021026e-09,\n",
       "       -1.33226763e-15,  2.50020982e-09,  2.50020982e-09, -4.99957853e-09])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 4\n",
    "target_evals = np.linalg.eigvalsh(xy_ham(n))\n",
    "print(target_evals)\n",
    "\n",
    "x = 4.47213595\n",
    "y = 1.23606798\n",
    "z = 2\n",
    "np.array([-x, -y-z, -y-z, -z, -y, -y, z-z, z-z, z-z, z-z, y, y, z, y+z, y+z, x]) - target_evals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "Sz = total_sz(n)\n",
    "S_sq = total_spin_squared(n)\n",
    "U = np.loadtxt(f'N={n}.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11, [4, 7, 10])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = 1\n",
    "m = -1\n",
    "\n",
    "j_cur = 0 if n % 2 == 0 else 1/2\n",
    "sector_start = 0 \n",
    "while j_cur < j: \n",
    "    sector_start += su2_multiplicity(n,j_cur) * su2_dimension(j_cur)\n",
    "    j_cur += 1\n",
    "\n",
    "sector_end = sector_start + su2_multiplicity(n,j) * su2_dimension(j)\n",
    "m_offset = int((-m * 2 + 2*j)/2) if (m*2)%2 == 1 else (-m+j)\n",
    "\n",
    "indices = []\n",
    "for i in range(su2_multiplicity(n,j)):\n",
    "    indices.append(sector_start + i*su2_dimension(j) + m_offset)\n",
    "\n",
    "sector_start, sector_end, indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.000000000000004+0j) (-2.0000000000000018+0j)\n",
      "(8.000000000000012+0j) (-2.0000000000000027+0j)\n",
      "(7.999999999999998+0j) (-1.9999999999999998+0j)\n"
     ]
    }
   ],
   "source": [
    "for i in indices: \n",
    "    print(U[:,i].conj().T @ S_sq @ U[:,i], U[:,i].conj().T @ Sz @ U[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = xxx_ham(10)\n",
    "evals, _ = np.linalg.eigh(ham) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrum_list = []\n",
    "spins = [0,1,2,3,4,5]\n",
    "for spin in spins:\n",
    "    with open(f'Q=10_S={spin}.txt', 'r') as file:\n",
    "        spectrum = [float(line.strip()) for line in file.readlines()]\n",
    "        spectrum_list.append(spectrum) \n",
    "\n",
    "sum([len(spectrum)*(2*spin+1) for spectrum,spin in zip(spectrum_list,spins)])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_spectrum = np.concatenate([spectrum*(2*spin+1) for spectrum,spin in zip(spectrum_list, spins)]) \n",
    "full_spectrum = np.sort(full_spectrum) \n",
    "np.allclose(evals, full_spectrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qaravan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
